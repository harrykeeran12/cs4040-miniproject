{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrykeeran12/cs4040-miniproject/blob/main/CS4040_Research_Methods_Miniproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXE83tQZmT9f"
      },
      "source": [
        "# A zero-shot evaluation of small speech-to-text systems\n",
        "\n",
        "The models evaluated are\n",
        "- the small Fairseq model,\n",
        "- the tiny Whisper model\n",
        "- and the medium Conformer-CTC model,\n",
        "due to their close parameter size.\n",
        "\n",
        "A small subset of the dataset is loaded first.\n",
        "Each step for the models is split up into:\n",
        "- Loading the models.\n",
        "- Translating the dataset into a form the model can understand using the processor.\n",
        "- Running each model on the dataset, to check if they can infer the data correctly.\n",
        "\n",
        "Then an evaluation is performed, with graphs.\n",
        "\n",
        "**This will require the T4 GPU to be enabled on Google Colab.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login\n",
        "!pip3 install torch torchvision torchaudio datasets transformers google.colab  datasets[audio] evaluate\n",
        "!pip3 install wget matplotlib>=3.3.2 text-unidecode\n",
        "\n",
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install Cython packaging\n",
        "!pip install \"nemo_toolkit[all]\"\n",
        "!pip install nemo_toolkit['asr']\n",
        "\n",
        "## Install NeMo\n",
        "# BRANCH = 'main'\n",
        "# !python -m pip install --upgrade --user git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]"
      ],
      "metadata": {
        "id": "_ZgGhYxpmX77",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbeh4X_gmT9g"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRzycAZImT9h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pprint import pprint\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration, WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset, IterableDataset, Audio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from evaluate import load\n",
        "from IPython.display import display\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nemo.collections.asr as nemo_asr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset(Mozilla Common Voice v17)"
      ],
      "metadata": {
        "id": "FlgQiAcZtqqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOMSEED = 42\n",
        "SAMPLE_NUMBER = 15\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "cv_17: IterableDataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"en\", split=\"validated\", streaming=True, trust_remote_code=True)\n",
        "\n",
        "cv_17 = cv_17.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
        "\n",
        "random10List : list[dict]= []\n",
        "iterdataset = iter(cv_17.shuffle(seed=RANDOMSEED, buffer_size = SAMPLE_NUMBER))\n",
        "for i in range(SAMPLE_NUMBER):\n",
        "  audio = next(iterdataset)\n",
        "  random10List.append(audio)\n",
        "  # pprint(audio)\n",
        "\n",
        "# pprint(random10List)"
      ],
      "metadata": {
        "id": "jBMEKVYmtup3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE9mKDGvmT9i"
      },
      "source": [
        "## Fairseq Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a3Fw8NqmT9i"
      },
      "outputs": [],
      "source": [
        "modelOne = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "processorOne = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "fairseqOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "  input_features = processorOne(\n",
        "      audio[\"audio\"][\"array\"],\n",
        "      sampling_rate=SAMPLING_RATE,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True\n",
        "  ).input_features\n",
        "  generated_ids = modelOne.generate(input_features=input_features)\n",
        "\n",
        "  transcription = processorOne.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  fairseqOutput[audio[\"sentence\"]] = transcription\n",
        "  print(f'{audio[\"sentence\"]} : {transcription}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T6-MDDYmT9j"
      },
      "source": [
        "## Whisper Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYMGiyJWmT9j"
      },
      "outputs": [],
      "source": [
        "# load model and processor\n",
        "processorTwo = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "modelTwo = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
        "modelTwo.config.forced_decoder_ids = None\n",
        "\n",
        "\n",
        "whisperOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "\n",
        "  # Whisper requires specific chunking requirements of 30 seconds for each item.\n",
        "  # The chunk size is represented by 30(seconds) times the sampling rate(16000)\n",
        "  chunk_size = 30 * SAMPLING_RATE\n",
        "  audioArray = audio[\"audio\"][\"array\"]\n",
        "\n",
        "\n",
        "\n",
        "  chunks = [audioArray[i:i + chunk_size] for i in range(0, len(audioArray), chunk_size)]\n",
        "  # Pads out the chunks that may be too small.\n",
        "  padded_chunks = [np.pad(chunk, (0, chunk_size - len(chunk)), 'constant', constant_values=0) for chunk in chunks if chunk.shape[0] < chunk_size]\n",
        "  # NOTE: Chunking may result in word errors at the boundary of the word.\n",
        "\n",
        "  transcriptions = []\n",
        "  for chunk in padded_chunks:\n",
        "    input_features = processorTwo(chunk, sampling_rate=SAMPLING_RATE,\n",
        "    return_tensors=\"pt\",padding=True).input_features\n",
        "    predicted_ids = modelTwo.generate(input_features)\n",
        "    transcription = processorTwo.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "    transcriptions.append(transcription)\n",
        "  # Joins each part of the chunked transcription together.\n",
        "  full_transcription = \" \".join(transcriptions)\n",
        "  whisperOutput[audio[\"sentence\"]] = full_transcription\n",
        "  print(f'{audio[\"sentence\"]} : {full_transcription}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BTohjPmT9j"
      },
      "source": [
        "## Medium Conformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nJ3lh9ZmT9k"
      },
      "outputs": [],
      "source": [
        "modelThree = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_conformer_ctc_medium\")\n",
        "\n",
        "\n",
        "CTCOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "  full_transcription = modelThree.transcribe(audio=audio[\"audio\"][\"array\"])[0]\n",
        "  CTCOutput[audio[\"sentence\"]] = full_transcription\n",
        "  print(f'{audio[\"sentence\"]} :  {full_transcription}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation:\n"
      ],
      "metadata": {
        "id": "_LL4tTbzvjjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fairseqList = []\n",
        "whisperList = []\n",
        "sentenceList = []\n",
        "CTCList = []\n",
        "\n",
        "fairseqWERList = []\n",
        "whisperWERList = []\n",
        "CTCWERList = []\n",
        "\n",
        "fairseqCERList = []\n",
        "whisperCERList = []\n",
        "CTCCERList = []\n",
        "\n",
        "# Loads the Word Error Rate Metric + the Character Error Rate Metric from HuggingFace.\n",
        "wer = load(\"wer\", module_type=\"metric\")\n",
        "cer = load(\"cer\", module_type=\"metric\")\n",
        "\n",
        "for audio in random10List:\n",
        "  # Gets the corresponding output for each sentence.\n",
        "  sentence = audio[\"sentence\"]\n",
        "\n",
        "  filteredSentence = re.sub(r'[^\\w\\s]','',sentence).lower()\n",
        "\n",
        "  fairseqTranscription = fairseqOutput[sentence].lower()\n",
        "  whisperTranscription = whisperOutput[sentence].lower()\n",
        "  CTCTranscription = CTCOutput[sentence].lower()\n",
        "  # Computes the WER for each model.\n",
        "  fairseqWER = wer.compute(predictions=[filteredSentence], references=[fairseqTranscription])\n",
        "  whisperWER = wer.compute(predictions=[filteredSentence], references=[whisperTranscription])\n",
        "  CTCWER = wer.compute(predictions=[filteredSentence], references=[CTCTranscription])\n",
        "\n",
        "  fairseqCER = cer.compute(predictions=[filteredSentence], references=[fairseqTranscription])\n",
        "  whisperCER = cer.compute(predictions=[filteredSentence], references=[whisperTranscription])\n",
        "  CTCCER = cer.compute(predictions=[filteredSentence], references=[CTCTranscription])\n",
        "\n",
        "\n",
        "  # Appends each sentence + transcribed sentence to a list to be added to the model later.\n",
        "  sentenceList.append(filteredSentence)\n",
        "  fairseqList.append(fairseqTranscription)\n",
        "  whisperList.append(whisperTranscription)\n",
        "  CTCList.append(CTCTranscription)\n",
        "  # Appends the WER to a list + rounds the number.\n",
        "  fairseqWERList.append(round(fairseqWER, 2))\n",
        "  whisperWERList.append(round(whisperWER, 2))\n",
        "  CTCWERList.append(round(CTCWER, 2))\n",
        "\n",
        "  # Appends the CER to a list + rounds the number.\n",
        "  fairseqCERList.append(round(fairseqCER, 2))\n",
        "  whisperCERList.append(round(whisperCER, 2))\n",
        "  CTCCERList.append(round(CTCCER, 2))\n",
        "\n",
        "\n",
        "transcriptionDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq Transcription\": fairseqList, \"Whisper Transcription\": whisperList, \"CTC-Conformer Transcription\" : CTCList})\n",
        "\n",
        "WERDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq WER\": fairseqWERList,\"Whisper WER\" : whisperWERList, \"CTC-Conformer WER\": CTCWERList})\n",
        "\n",
        "CERDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq CER\": fairseqCERList,\"Whisper CER\" : whisperCERList, \"CTC-Conformer CER\": CTCCERList})\n",
        "\n",
        "display(WERDF)\n",
        "display(CERDF)"
      ],
      "metadata": {
        "id": "Vzu-P3eDKc7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create bar plots for each model's WER\n",
        "plt.bar(WERDF.index, WERDF[\"Fairseq WER\"], label=\"Fairseq\", width=0.25)\n",
        "plt.bar(WERDF.index + 0.25, WERDF[\"Whisper WER\"], label=\"Whisper\", width=0.25)\n",
        "plt.bar(WERDF.index + 0.5, WERDF[\"CTC-Conformer WER\"], label=\"CTC-Conformer\", width=0.25)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Sentence Number\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.title(\"WER Rates for Each Sentence\")\n",
        "plt.xticks(WERDF.index + 0.25, WERDF.index) # adjust x-axis ticks\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vmyIFw5m35_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create bar plots for each model's CER\n",
        "plt.bar(CERDF.index, CERDF[\"Fairseq CER\"], label=\"Fairseq\", width=0.25)\n",
        "plt.bar(CERDF.index + 0.25, CERDF[\"Whisper CER\"], label=\"Whisper\", width=0.25)\n",
        "plt.bar(CERDF.index + 0.5, CERDF[\"CTC-Conformer CER\"], label=\"CTC-Conformer\", width=0.25)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Sentence Number\")\n",
        "plt.ylabel(\"CER\")\n",
        "plt.title(\"CER Rates for Each Sentence\")\n",
        "plt.xticks(CERDF.index + 0.25, CERDF.index) # adjust x-axis ticks\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JWfbsAD3rlgl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FlgQiAcZtqqY",
        "tE9mKDGvmT9i",
        "5T6-MDDYmT9j"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}