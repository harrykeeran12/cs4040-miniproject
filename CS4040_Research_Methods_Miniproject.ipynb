{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harrykeeran12/cs4040-miniproject/blob/main/CS4040_Research_Methods_Miniproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXE83tQZmT9f"
      },
      "source": [
        "# A zero-shot evaluation of small speech-to-text systems\n",
        "\n",
        "The models evaluated are\n",
        "- the small Fairseq model,\n",
        "- the tiny Whisper model\n",
        "- and the medium Conformer-CTC model,\n",
        "due to their close parameter size.\n",
        "\n",
        "A small subset of the dataset is loaded first.\n",
        "Each step for the models is split up into:\n",
        "- Loading the models.\n",
        "- Translating the dataset into a form the model can understand using the processor.\n",
        "- Running each model on the dataset, to check if they can infer the data correctly.\n",
        "\n",
        "Then an evaluation is performed, with graphs.\n",
        "\n",
        "**This will require the T4 GPU to be enabled on Google Colab.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login\n",
        "!pip3 install torch torchvision torchaudio datasets transformers google.colab  datasets[audio] evaluate\n",
        "!pip3 install wget matplotlib>=3.3.2 text-unidecode\n",
        "\n",
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install Cython packaging\n",
        "!pip install \"nemo_toolkit[all]\"\n",
        "!pip install nemo_toolkit['asr']\n",
        "\n",
        "## Install NeMo\n",
        "# BRANCH = 'main'\n",
        "# !python -m pip install --upgrade --user git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[all]"
      ],
      "metadata": {
        "id": "_ZgGhYxpmX77",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbeh4X_gmT9g"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRzycAZImT9h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pprint import pprint\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration, WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset, IterableDataset, Audio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import scipy.stats as stats\n",
        "from evaluate import load\n",
        "from IPython.display import display\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nemo.collections.asr as nemo_asr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset(Mozilla Common Voice v17)"
      ],
      "metadata": {
        "id": "FlgQiAcZtqqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOMSEED = 42\n",
        "SAMPLE_NUMBER = 15\n",
        "SAMPLING_RATE = 16000\n",
        "\n",
        "cv_17: IterableDataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"en\", split=\"validated\", streaming=True, trust_remote_code=True)\n",
        "\n",
        "cv_17 = cv_17.cast_column(\"audio\", Audio(sampling_rate=SAMPLING_RATE))\n",
        "\n",
        "random10List : list[dict]= []\n",
        "iterdataset = iter(cv_17.shuffle(seed=RANDOMSEED, buffer_size = SAMPLE_NUMBER))\n",
        "for i in range(SAMPLE_NUMBER):\n",
        "  audio = next(iterdataset)\n",
        "  random10List.append(audio)\n",
        "  # pprint(audio)\n",
        "\n",
        "# pprint(random10List)"
      ],
      "metadata": {
        "id": "jBMEKVYmtup3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE9mKDGvmT9i"
      },
      "source": [
        "## Fairseq Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a3Fw8NqmT9i"
      },
      "outputs": [],
      "source": [
        "modelOne = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "processorOne = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "\n",
        "fairseqOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "  input_features = processorOne(\n",
        "      audio[\"audio\"][\"array\"],\n",
        "      sampling_rate=SAMPLING_RATE,\n",
        "      return_tensors=\"pt\",\n",
        "      padding=True\n",
        "  ).input_features\n",
        "  generated_ids = modelOne.generate(input_features=input_features)\n",
        "\n",
        "  transcription = processorOne.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  fairseqOutput[audio[\"sentence\"]] = transcription\n",
        "  print(f'{audio[\"sentence\"]} : {transcription}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T6-MDDYmT9j"
      },
      "source": [
        "## Whisper Tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYMGiyJWmT9j"
      },
      "outputs": [],
      "source": [
        "# load model and processor\n",
        "processorTwo = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
        "modelTwo = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
        "modelTwo.config.forced_decoder_ids = None\n",
        "\n",
        "\n",
        "whisperOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "\n",
        "  # Whisper requires specific chunking requirements of 30 seconds for each item.\n",
        "  # The chunk size is represented by 30(seconds) times the sampling rate(16000)\n",
        "  chunk_size = 30 * SAMPLING_RATE\n",
        "  audioArray = audio[\"audio\"][\"array\"]\n",
        "\n",
        "\n",
        "\n",
        "  chunks = [audioArray[i:i + chunk_size] for i in range(0, len(audioArray), chunk_size)]\n",
        "  # Pads out the chunks that may be too small.\n",
        "  padded_chunks = [np.pad(chunk, (0, chunk_size - len(chunk)), 'constant', constant_values=0) for chunk in chunks if chunk.shape[0] < chunk_size]\n",
        "  # NOTE: Chunking may result in word errors at the boundary of the word.\n",
        "\n",
        "  transcriptions = []\n",
        "  for chunk in padded_chunks:\n",
        "    input_features = processorTwo(chunk, sampling_rate=SAMPLING_RATE,\n",
        "    return_tensors=\"pt\",padding=True).input_features\n",
        "    predicted_ids = modelTwo.generate(input_features)\n",
        "    transcription = processorTwo.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "    transcriptions.append(transcription)\n",
        "  # Joins each part of the chunked transcription together.\n",
        "  full_transcription = \" \".join(transcriptions)\n",
        "  whisperOutput[audio[\"sentence\"]] = full_transcription\n",
        "  print(f'{audio[\"sentence\"]} : {full_transcription}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BTohjPmT9j"
      },
      "source": [
        "## Medium Conformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nJ3lh9ZmT9k"
      },
      "outputs": [],
      "source": [
        "modelThree = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_conformer_ctc_medium\")\n",
        "\n",
        "\n",
        "CTCOutput :dict[str,str]= {}\n",
        "for audio in random10List:\n",
        "  full_transcription = modelThree.transcribe(audio=audio[\"audio\"][\"array\"])[0]\n",
        "  CTCOutput[audio[\"sentence\"]] = full_transcription\n",
        "  print(f'{audio[\"sentence\"]} :  {full_transcription}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation:\n"
      ],
      "metadata": {
        "id": "_LL4tTbzvjjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fairseqList = []\n",
        "whisperList = []\n",
        "sentenceList = []\n",
        "CTCList = []\n",
        "\n",
        "fairseqWERList = []\n",
        "whisperWERList = []\n",
        "CTCWERList = []\n",
        "\n",
        "fairseqCERList = []\n",
        "whisperCERList = []\n",
        "CTCCERList = []\n",
        "\n",
        "SIG_FIG = 4\n",
        "\n",
        "# Loads the Word Error Rate Metric + the Character Error Rate Metric from HuggingFace.\n",
        "wer = load(\"wer\", module_type=\"metric\")\n",
        "cer = load(\"cer\", module_type=\"metric\")\n",
        "\n",
        "for audio in random10List:\n",
        "  # Gets the corresponding output for each sentence.\n",
        "  sentence = audio[\"sentence\"]\n",
        "\n",
        "  filteredSentence = re.sub(r'[^\\w\\s]','',sentence).lower()\n",
        "\n",
        "  fairseqTranscription = fairseqOutput[sentence].lower()\n",
        "  whisperTranscription = whisperOutput[sentence].lower()\n",
        "  CTCTranscription = CTCOutput[sentence].lower()\n",
        "  # Computes the WER for each model.\n",
        "  fairseqWER = wer.compute(predictions=[filteredSentence], references=[fairseqTranscription])\n",
        "  whisperWER = wer.compute(predictions=[filteredSentence], references=[whisperTranscription])\n",
        "  CTCWER = wer.compute(predictions=[filteredSentence], references=[CTCTranscription])\n",
        "\n",
        "  fairseqCER = cer.compute(predictions=[filteredSentence], references=[fairseqTranscription])\n",
        "  whisperCER = cer.compute(predictions=[filteredSentence], references=[whisperTranscription])\n",
        "  CTCCER = cer.compute(predictions=[filteredSentence], references=[CTCTranscription])\n",
        "\n",
        "\n",
        "  # Appends each sentence + transcribed sentence to a list to be added to the model later.\n",
        "  sentenceList.append(filteredSentence)\n",
        "  fairseqList.append(fairseqTranscription)\n",
        "  whisperList.append(whisperTranscription)\n",
        "  CTCList.append(CTCTranscription)\n",
        "  # Appends the WER to a list + rounds the number.\n",
        "  fairseqWERList.append(round(fairseqWER, SIG_FIG))\n",
        "  whisperWERList.append(round(whisperWER, SIG_FIG))\n",
        "  CTCWERList.append(round(CTCWER, SIG_FIG))\n",
        "\n",
        "  # Appends the CER to a list + rounds the number.\n",
        "  fairseqCERList.append(round(fairseqCER, SIG_FIG))\n",
        "  whisperCERList.append(round(whisperCER, SIG_FIG))\n",
        "  CTCCERList.append(round(CTCCER, SIG_FIG))\n",
        "\n",
        "\n",
        "transcriptionDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq Transcription\": fairseqList, \"Whisper Transcription\": whisperList, \"CTC-Conformer Transcription\" : CTCList})\n",
        "\n",
        "WERDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq WER\": fairseqWERList,\"Whisper WER\" : whisperWERList, \"CTC-Conformer WER\": CTCWERList})\n",
        "\n",
        "CERDF = pd.DataFrame({\"Sentence(after preprocessing)\": sentenceList, \"Fairseq CER\": fairseqCERList,\"Whisper CER\" : whisperCERList, \"CTC-Conformer CER\": CTCCERList})\n",
        "\n",
        "display(WERDF)\n",
        "display(CERDF)"
      ],
      "metadata": {
        "id": "Vzu-P3eDKc7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WER for each sentence"
      ],
      "metadata": {
        "id": "tCHQMw7f1aLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create bar plots for each model's WER\n",
        "plt.bar(WERDF.index, WERDF[\"Fairseq WER\"], label=\"Fairseq\", width=0.25)\n",
        "plt.bar(WERDF.index + 0.25, WERDF[\"Whisper WER\"], label=\"Whisper\", width=0.25)\n",
        "plt.bar(WERDF.index + 0.5, WERDF[\"CTC-Conformer WER\"], label=\"CTC-Conformer\", width=0.25)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Sentence Number\")\n",
        "plt.ylabel(\"WER\")\n",
        "plt.title(\"WER Rates for Each Sentence\")\n",
        "plt.xticks(WERDF.index + 0.25, WERDF.index) # adjust x-axis ticks\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9vmyIFw5m35_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CER for each sentence"
      ],
      "metadata": {
        "id": "A4-4nwDp1fSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create bar plots for each model's CER\n",
        "plt.bar(CERDF.index, CERDF[\"Fairseq CER\"], label=\"Fairseq\", width=0.25)\n",
        "plt.bar(CERDF.index + 0.25, CERDF[\"Whisper CER\"], label=\"Whisper\", width=0.25)\n",
        "plt.bar(CERDF.index + 0.5, CERDF[\"CTC-Conformer CER\"], label=\"CTC-Conformer\", width=0.25)\n",
        "\n",
        "\n",
        "plt.xlabel(\"Sentence Number\")\n",
        "plt.ylabel(\"CER\")\n",
        "plt.title(\"CER Rates for Each Sentence\")\n",
        "plt.xticks(CERDF.index + 0.25, CERDF.index) # adjust x-axis ticks\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JWfbsAD3rlgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Median WER + CER for each model"
      ],
      "metadata": {
        "id": "-lp4KSm81jOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"WER means: \\n\")\n",
        "print(WERDF[\"Fairseq WER\"].mean())\n",
        "print(WERDF[\"Whisper WER\"].mean())\n",
        "print(WERDF[\"CTC-Conformer WER\"].mean())\n",
        "print(\"WER ranges: \\n\")\n",
        "print(f'{WERDF[\"Fairseq WER\"].min()}, {WERDF[\"Fairseq WER\"].max()}')\n",
        "print(f'{WERDF[\"Whisper WER\"].min()}, {WERDF[\"Whisper WER\"].max()}')\n",
        "print(f'{WERDF[\"CTC-Conformer WER\"].min()}, {WERDF[\"CTC-Conformer WER\"].max()}')\n",
        "print(\"CER means: \\n\")\n",
        "print(CERDF[\"Fairseq CER\"].mean())\n",
        "print(CERDF[\"Whisper CER\"].mean())\n",
        "print(CERDF[\"CTC-Conformer CER\"].mean())\n",
        "print(\"CER ranges: \\n\")\n",
        "print(CERDF[\"Fairseq CER\"].min(), CERDF[\"Fairseq CER\"].max())\n",
        "print(CERDF[\"Whisper CER\"].min(), CERDF[\"Whisper CER\"].max())\n",
        "print(CERDF[\"CTC-Conformer CER\"].min(), CERDF[\"CTC-Conformer CER\"].max())"
      ],
      "metadata": {
        "id": "k5jcPobdOmG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "models = [\"Fairseq\", \"Whisper\", \"CTC-Conformer\"]\n",
        "WERmedians = [WERDF[\"Fairseq WER\"].median(), WERDF[\"Whisper WER\"].median(), WERDF[\"CTC-Conformer WER\"].median()]\n",
        "CERmedians = [CERDF[\"Fairseq CER\"].median(), CERDF[\"Whisper CER\"].median(), CERDF[\"CTC-Conformer CER\"].median()]\n",
        "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
        "\n",
        "ax[0].bar(models, WERmedians, color=colors)\n",
        "ax[0].set_xlabel(\"Median Name\")\n",
        "ax[0].set_ylabel(\"Median WER(lowest is better)\")\n",
        "ax[0].set_title(\"Median WER per Model\")\n",
        "# ax[0].set_ylim(0, 0.5)\n",
        "\n",
        "ax[1].bar(models, CERmedians, color=colors)\n",
        "ax[1].set_xlabel(\"Model Name\")\n",
        "ax[1].set_ylabel(\"Median CER(lowest is better)\")\n",
        "ax[1].set_title(\"Median CER per Model\")\n",
        "# ax[1].set_ylim(0, 0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NDptPlB31nMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Analysis\n"
      ],
      "metadata": {
        "id": "ybMFBsiEEXLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine WER data for all models\n",
        "wer_data = [WERDF[\"Fairseq WER\"], WERDF[\"Whisper WER\"], WERDF[\"CTC-Conformer WER\"]]\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(*wer_data)\n",
        "\n",
        "print(f\"ANOVA test for WER:\")\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Combine CER data for all models\n",
        "cer_data = [CERDF[\"Fairseq CER\"], CERDF[\"Whisper CER\"], CERDF[\"CTC-Conformer CER\"]]\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value = stats.f_oneway(*cer_data)\n",
        "\n",
        "print(f\"\\nANOVA test for CER:\")\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GATyU9SkEbnK",
        "outputId": "bf39ab2b-46e6-4f32-c561-5f6298e6fb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA test for WER:\n",
            "F-statistic: 3.932834919478482\n",
            "P-value: 0.027189442100556207\n",
            "\n",
            "ANOVA test for CER:\n",
            "F-statistic: 2.9251679964762847\n",
            "P-value: 0.06466216986778027\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tE9mKDGvmT9i",
        "5T6-MDDYmT9j",
        "40BTohjPmT9j"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}